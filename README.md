# Awesome Multimodal Embedding Models

这个仓库收集和总结了目前效果最好的多模态嵌入模型（Multimodal Embedding Models）。这些模型能够将不同模态的数据（如文本、图像、音频等）映射到同一个向量空间中，使得跨模态的相似度计算和检索成为可能。


## 目录

- [模型列表](#模型列表)
- [评估指标](#评估指标)
- [应用场景](#应用场景)
- [贡献指南](#贡献指南)

## 模型列表

### 文本-图像模型
- CLIP (Contrastive Language-Image Pre-training)
- OpenCLIP
- CoCa (Contrastive Captioner)
- BLIP-2
- Flamingo

### 文本-音频模型
- CLAP (Contrastive Language-Audio Pre-training)
- AudioCLIP

### 多模态模型
- ImageBind
- CoDi
- Unified-IO

## 评估指标

评估多模态嵌入模型的主要指标包括：

- 跨模态检索准确率（Cross-modal Retrieval Accuracy）
- 零样本分类准确率（Zero-shot Classification Accuracy）
- 语义相似度相关性（Semantic Similarity Correlation）
- 计算效率（Computational Efficiency）
- 模型大小（Model Size）

## 应用场景

多模态嵌入模型在以下场景中发挥着重要作用：

1. 跨模态检索
   - 文本到图像搜索
   - 图像到文本搜索
   - 音频到文本搜索

2. 零样本分类
   - 图像分类
   - 音频分类
   - 视频分类

3. 多模态内容理解
   - 图像描述生成
   - 视频理解
   - 多模态问答

4. 推荐系统
   - 跨模态内容推荐
   - 个性化搜索

## 贡献指南

欢迎贡献新的模型、评估结果或改进建议！请遵循以下步骤：

1. Fork 本仓库
2. 创建新的分支
3. 提交你的更改
4. 发起 Pull Request

## 许可证

MIT License

## 致谢

感谢所有开源社区的研究者和开发者，他们的工作为多模态AI领域做出了重要贡献。 